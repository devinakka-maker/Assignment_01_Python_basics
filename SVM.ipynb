{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "\n",
        "Information Gain measures how much \"uncertainty\" (entropy) is reduced when a dataset is split on a particular feature. In decision trees, it is used to decide which feature to split on at each step, ensuring the tree becomes more accurate and efficient by creating purer subsets.\n",
        "\n",
        "\n",
        "Decision Trees build classification or regression models by splitting data into subsets. Information Gain guides this process:\n",
        "- Calculate Entropy of the Dataset\n",
        "   - Example: If we’re classifying animals into \"Mammal\" vs. \"Bird,\" entropy is high if the dataset is mixed.\n",
        "- Evaluate Each Feature\n",
        "   - For each candidate feature (e.g., \"Has Wings,\" \"Gives Milk\"), calculate the Information Gain.\n",
        "- Choose the Best Split\n",
        "   - The feature with the highest Information Gain is chosen because it reduces uncertainty the most.\n",
        "- Repeat Recursively\n",
        "   - The process continues until subsets are pure (entropy = 0) or stopping criteria are met (e.g., max depth).\n"
      ],
      "metadata": {
        "id": "4JHIeVYCakeh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Both Gini Impurity and Entropy are measures of how mixed a dataset is, used in decision trees to decide the best split. The key difference is that Entropy uses logarithms to measure disorder, while Gini Impurity uses squared probabilities.\n",
        "- Gini:\n",
        "   - It measures the probability that a randomly chosen sample would be misclassified if you assign its label according to the distribution of classes in the node.\n",
        "   - id gini=0.48 it means: if you randomly pick a sample and assign its class based on the node’s distribution, there’s a 48% chance that this assignment will be wrong.\n",
        "- Entropy:\n",
        "  -  It tells us how much information is required, on average, to describe the outcome of a random process\n",
        "   - Entropy = 0.98 means the node is almost at maximum uncertainty, so nearly the highest amount of information is needed to resolve it\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wh9-Zjgve9_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What is Pre-Pruning in Decision Trees?\n",
        "- Pre-pruning (also called early stopping) is a method where the decision tree growth is stopped before it becomes overly complex. Instead of building a full tree and then trimming it, pre-pruning sets constraints during tree construction.\n",
        "- To prevent overfitting by limiting unnecessary splits that don’t significantly improve predictive accuracy\n",
        "- Decision tree algorithms use thresholds to decide when to stop splitting:\n",
        "  - Maximum Depth: Restrict how deep the tree can grow.\n",
        "  - Minimum Samples per Node: Stop splitting if a node has fewer than a set number of samples.\n",
        "  - Minimum Information Gain: Only split if the gain in accuracy (e.g., reduction in impurity) exceeds a threshold\n"
      ],
      "metadata": {
        "id": "9S5wOC4Oo4--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "4.Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_\n",
        "'''\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Display feature importances in a neat format\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Feature Importances using Gini Impurity:\")\n",
        "print(importance_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnEelRFSqMXy",
        "outputId": "0692a3a9-22ff-4a23-f619-710558879dd8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances using Gini Impurity:\n",
            "             Feature  Importance\n",
            "2  petal length (cm)    0.564056\n",
            "3   petal width (cm)    0.422611\n",
            "0  sepal length (cm)    0.013333\n",
            "1   sepal width (cm)    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What is a Support Vector Machine (SVM)?\n",
        " - A Support Vector Machine (SVM) is a supervised machine learning algorithm that finds the optimal boundary (called a hyperplane) to separate data into different classes with the maximum margin.\n",
        "-  Idea of SVM\n",
        "    - Classification: SVM tries to separate data points of different classes using a hyperplane.\n",
        "    - Maximum Margin Principle: The chosen hyperplane is the one that maximizes the distance (margin) between itself and the nearest data points from each class. These nearest points are called support vectors.\n",
        "    - Generalization: By maximizing the margin, SVM reduces the risk of misclassification on unseen data.\n",
        "  -  Linear Case: If data is linearly separable, SVM finds a straight line (in 2D), plane (in 3D), or hyperplane (in higher dimensions) that best separates the classes.\n",
        "- Nonlinear Case: If data isn’t linearly separable, SVM uses the kernel trick to project data into a higher-dimensional space where a linear separator can be found.\n",
        "- Common kernels: Linear, Polynomial, Radial Basis Function (RBF), Sigmoid.\n",
        "- Support Vectors: Only a subset of training points (the ones closest to the boundary) influence the decision function, making SVM memory-efficient\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RLPCZvxTrybM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What is the Kernel Trick in SVM?\n",
        "- The Kernel Trick in SVM is a mathematical technique that allows Support Vector Machines to handle non-linear data by implicitly mapping it into a higher-dimensional space without explicitly computing the transformation\n",
        "- Kernel Trick is Needed\n",
        "  - Linear separability: SVMs work best when data can be separated by a straight line (or hyperplane).\n",
        "  - Real-world data: Often, data is non-linear and cannot be separated with a simple line.\n",
        "  - Solution: Map the data into a higher-dimensional space where it becomes linearly separable.\n",
        "- How the Kernel Trick Works:\n",
        "  - Instead of explicitly computing the coordinates in higher dimensions, SVM uses a kernel function to calculate the inner product between two points in that space.\n",
        "  - This avoids the computational cost of working directly in high dimensions.\n",
        "  - Key idea: You never need to know the actual mapping; the kernel function does the job implicitly\n",
        "\n",
        "comman kernals are:\n",
        "linear(Basic one)\n",
        "polynomial\n",
        "gaussian(RBF)\n",
        "sigmoid\n",
        "\n"
      ],
      "metadata": {
        "id": "c9M-ao1AtC1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "7.\n",
        "Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset.\n",
        "'''\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "\n",
        "print(\"Accuracy with Linear Kernel:\", accuracy_linear)\n",
        "print(\"Accuracy with RBF Kernel:\", accuracy_rbf)\n",
        "\n",
        "if accuracy_linear > accuracy_rbf:\n",
        "    print(\"Linear kernel performed better.\")\n",
        "elif accuracy_rbf > accuracy_linear:\n",
        "    print(\"RBF kernel performed better.\")\n",
        "else:\n",
        "    print(\"Both kernels performed equally well.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOg2VZ8prpwY",
        "outputId": "f8d3e685-a2e3-43d4-cfb0-4a1d1075dec3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Linear Kernel: 0.9444444444444444\n",
            "Accuracy with RBF Kernel: 0.6944444444444444\n",
            "Linear kernel performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DAyb3St9s2bG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "- Bayes’ Theorem: It calculates the probability of a class given the features:\n",
        "P(C|X)={P(X|C)*P(C)}/{P(X)}\n",
        "- where:\n",
        "  - P(C|X): Posterior probability of class C given features X\n",
        "  - P(X|C): Likelihood of features given class C\n",
        "  - P(C): Prior probability of class C\n",
        "  - P(X): Evidence (probability of features)\n",
        "  - Classifier: Naïve Bayes uses this theorem to assign the most probable class to a given input.\n",
        "- Bayes’ Theorem: General rule, works with dependent events.\n",
        "- Naïve Bayes classifier: Applies Bayes’ Theorem but assumes independence between features to make computation easier.\n",
        "- That’s why it’s called “naïve” — it ignores dependencies\n"
      ],
      "metadata": {
        "id": "WWl-_pNHwOOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "- 1. Gaussian Naïve Bayes\n",
        "   - Assumption: Features are continuous and follow a normal (Gaussian) distribution.\n",
        "   - Use Case: Works well for datasets with continuous numerical values (e.g., height, weight, sensor readings).\n",
        "- 2. Multinomial Naïve Bayes- Assumption: Features represent discrete counts (non-negative integers).\n",
        "  - Use Case: Commonly used in text classification (spam detection, sentiment analysis) where features are word counts or term frequencies.\n",
        "- 3. Bernoulli Naïve Bayes- Assumption: Features are binary (0 or 1), representing presence/absence of a feature.\n",
        "  - Use Case: Also used in text classification, but instead of word counts, it considers whether a word appears at all(Yes/No).\n"
      ],
      "metadata": {
        "id": "5y-HEZssyPq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "10.\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets\n",
        "\n",
        "'''\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy of Gaussian Naïve Bayes on Breast Cancer dataset:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqKMAOlBwFLi",
        "outputId": "4c2bbfef-f3f6-4e61-9270-a5922858eaba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes on Breast Cancer dataset: 0.9385964912280702\n"
          ]
        }
      ]
    }
  ]
}