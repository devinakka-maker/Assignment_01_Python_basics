{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "- K-Nearest Neighbors (KNN) is a simple, non-parametric algorithm that predicts outcomes based on the closest data points. In classification, it assigns the majority class among neighbors, while in regression, it averages their values.\n",
        "- KNN in Classification\n",
        "  - Compute distances between the new point and all training points.\n",
        "  - Select the k nearest neighbors.\n",
        "  - Assign the class label by majority vote among neighbors.\n",
        "- KNN in Regression\n",
        "  - Compute distances to all training points.\n",
        "  - Select the k nearest neighbors.\n",
        "  - Predict the target as the average (or weighted average) of neighbors’ values.\n",
        "\n"
      ],
      "metadata": {
        "id": "vhGA2sNUUu7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "- It’s a term used in machine learning to describe problems that occur when your dataset has too many features (dimensions).\n",
        "- As the number of dimensions increases:\n",
        "- Data points become sparse (spread out).\n",
        "- Distances lose meaning — in high dimensions, all points start to look equally far from each other.\n",
        "- Algorithms that rely on distance or density (like KNN) struggle.\n",
        "- KNN works by finding the nearest neighbors to a point using a distance measure (like Euclidean distance).\n",
        "- But in high dimensions:\n",
        "  - Nearest vs farthest neighbor gap shrinks\n",
        "  - In 2D, the closest neighbor is clearly nearer than the farthest.\n",
        "  - In 100D, the difference between “closest” and “farthest” becomes very small.\n",
        "  - Result: KNN can’t reliably tell who is truly close.\n",
        "  - Noise dominates\n",
        "  - Extra irrelevant features add random variation.\n",
        "  - KNN treats all features equally, so important signals get drowned out.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6i7jNT0QVww6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "-  PCA is a dimensionality reduction technique.\n",
        "- It transforms your original features into a new set of variables called principal components.\n",
        "- These components are:\n",
        "- Linear combinations of the original features.\n",
        "- Ordered so that the first component captures the most variance in the data, the second captures the next most, and so on.\n",
        "- The goal: reduce the number of dimensions while keeping as much information (variance) as possible.\n",
        "- Feature Selection\n",
        "  - Chooses a subset of the original features and discards the rest.\n",
        "  - Goal: Keep only the most relevant predictors for the target variable (or reduce redundancy).\n",
        "  - Nature: Can be supervised (using target labels) or unsupervised.\n",
        "  - Interpretability: Easier, because you’re still working with the original features\n",
        "\n"
      ],
      "metadata": {
        "id": "u74o8zzXeaxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "- Eigenvectors\n",
        "  - They represent the directions of maximum variance in the data.\n",
        "  - In PCA, each eigenvector corresponds to a principal component.\n",
        "- Eigenvalues\n",
        "  - They represent the amount of variance captured by each eigenvector.\n",
        "  - Larger eigenvalue = more information (variance) explained by that component.\n",
        "-  Identify principal components\n",
        "   - Eigenvectors define the new axes (principal components) onto which data is projected.\n",
        "   - Rank importance of components\n",
        "   Eigenvalues tell us how much variance each component explains.\n",
        "   - This helps decide how many components to keep.\n",
        "   - Dimensionality reduction\n",
        "By keeping only the top k eigenvectors, we reduce dimensions while preserving most of the information.\n",
        "   - Noise filtering\n",
        "Components with very small eigenvalues capture little variance . Dropping them improves model efficiency and generalization.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tuAsMDSAinTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "- KNN relies on distance (Euclidean, Manhattan, etc.) to find nearest neighbors.\n",
        "- In high-dimensional data, distances become unreliable due to the Curse of Dimensionality (all points start to look equally far apart).\n",
        "- This hurts KNN’s performance, especially when many features are noisy or redundant.\n",
        "- PCA reduces dimensionality by projecting data onto fewer principal components that capture most of the variance.\n",
        "- This has several benefits for KNN:\n",
        "- Removes noise & redundancy - KNN focuses on meaningful features.\n",
        "- Improves distance reliability - Distances in lower dimensions are more discriminative.\n",
        "- Speeds up computation - Fewer dimensions mean faster distance calculations.\n",
        "- Mitigates overfitting - KNN generalizes better when irrelevant features are removed\n",
        "\n"
      ],
      "metadata": {
        "id": "IPmeNQU-kxcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "6.Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = knn_scaling.predict(X_test_scaled)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "\n",
        "print(\"Accuracy WITHOUT scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy WITH scaling   :\", accuracy_scaling)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m54pFFX_60sD",
        "outputId": "808eec06-76ef-43a2-b1c6-0c71b0e06e9b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT scaling: 0.7222222222222222\n",
            "Accuracy WITH scaling   : 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "7.Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "\n",
        "print(\"Explained variance ratio of each principal component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {ratio:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZxwa-Wr7MpC",
        "outputId": "0717caed-338d-4f18-c2d7-0e1d88f509ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each principal component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "8.Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "\n",
        "print(\"Accuracy on Original Dataset:\", accuracy_original)\n",
        "print(\"Accuracy on PCA (2 components):\", accuracy_pca)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUCSpFOC7en7",
        "outputId": "916d8b11-63a5-49dc-c669-dddf26cb893b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Original Dataset: 0.9444444444444444\n",
            "Accuracy on PCA (2 components): 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "9.Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "\n",
        "print(\"Accuracy with Euclidean distance:\", accuracy_euclidean)\n",
        "print(\"Accuracy with Manhattan distance:\", accuracy_manhattan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeUS7b0p7z2r",
        "outputId": "8b3f2350-0187-4dad-da38-0f07faa3402f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 0.9444444444444444\n",
            "Accuracy with Manhattan distance: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YeHpO-yXofEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "10.You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical dat\n",
        "'''\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=100, n_features=5000,\n",
        "                           n_informative=50, n_classes=3, random_state=42)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "\n",
        "cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
        "n_components = np.argmax(cum_var >= 0.95) + 1\n",
        "print(f\"Number of components to retain (95% variance): {n_components}\")\n",
        "\n",
        "\n",
        "pca = PCA(n_components=n_components)\n",
        "X_reduced = pca.fit_transform(X_scaled)\n",
        "\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(knn, X_reduced, y, cv=cv, scoring='accuracy')\n",
        "\n",
        "print(\"Cross-validation accuracies:\", scores)\n",
        "print(\"Mean accuracy:\", scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1InVbgqy8Jhg",
        "outputId": "73ae8acb-b141-453b-f2fe-01abe277e00c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of components to retain (95% variance): 93\n",
            "Cross-validation accuracies: [0.4  0.4  0.2  0.3  0.35]\n",
            "Mean accuracy: 0.32999999999999996\n"
          ]
        }
      ]
    }
  ]
}